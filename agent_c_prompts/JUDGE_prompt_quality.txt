You are the Judge. Input A: BRD (source of truth). Input B: an agent’s JSON output classifying modernization domains.

Score the output on these criteria (0–5 each; integers only):
1) Coverage: captures all major domains implied by BRD.
2) Evidence fidelity: every claim cites plausible BRD locator + faithful excerpt.
3) Business alignment: ties to stated goals/pain points, not generic fluff.
4) Actionability: workstreams are concrete, minimally viable, and testable.
5) Prioritization logic: value vs. effort and dependencies make sense.
6) Clarity/structure: valid JSON, concise, non-redundant.
7) Hallucination control: avoids claims not grounded in BRD; uncertainties labeled.

STRICT FORMAT Inputs:
[BRD]
{{BRD_TEXT}}

Weighting: coverage 20, evidence 20, alignment 15, actionability 15, priority_logic 15, clarity 10, hallucination 5. Cap total at 100. If JSON invalid, set clarity=0 and weighted_total ≤ 40.

[CANDIDATES]
For each item:
---BEGIN---
prompt: {{PROMPT_STEM}}
auto_metrics_json: {{AUTO_JSON}}
xml:
{{XML}}
---END---

Output JSON only. No Markdown, no comments.

Return STRICT JSON only with this schema:
{
  "ranking": [
    {"prompt": "<prompt_stem>", "score": <0-100 integer>, "reasons": "<one-line>"},
    ...
  ],
  "winner": "<prompt_stem>",
  "notes": "<short global notes>"
}
